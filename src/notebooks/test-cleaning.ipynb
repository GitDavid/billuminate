{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().config.get('IPKernelApp', {})['parent_appname'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.platform == \"linux\":\n",
    "    sys.path.append('../')\n",
    "\n",
    "    MODEL_ROOT = '../../models/'\n",
    "    NLP_MODEL_ROOT = '../../nlp_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/swimmers3/ferrari_06/repo/billuminate/src/notebooks'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[34;42mconda_envs\u001b[0m/                 \u001b[01;32mdump.sql\u001b[0m*    \u001b[01;32mREADME.md\u001b[0m*\n",
      "\u001b[01;32mcongress_bills_schema.sql\u001b[0m*  \u001b[34;42mnlp_models\u001b[0m/  \u001b[34;42msrc\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import sqlalchemy\n",
    "import sqlalchemy_utils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "import string\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [File Schema Explanation](https://github.com/usgpo/bill-status/blob/master/BILLSTATUS-XML_User_User-Guide.md#3.-Action-Code-Element-Possible-Values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_rows(df, n_rows):\n",
    "    ixs = np.random.choice(df.index.values, n_rows)\n",
    "    df = df.reindex(ixs)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgres://melissaferrari@localhost/congressional_bills\n"
     ]
    }
   ],
   "source": [
    "# Connect to db wiht sqlalchemy\n",
    "dbname = 'congressional_bills'\n",
    "username = 'melissaferrari'\n",
    "engine = sqlalchemy.create_engine('postgres://%s@localhost/%s'%(username,dbname))\n",
    "print(engine.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bills_info = pd.read_sql_table('bills', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>official_title</th>\n",
       "      <th>popular_title</th>\n",
       "      <th>url</th>\n",
       "      <th>bill_type</th>\n",
       "      <th>status_at</th>\n",
       "      <th>by_request</th>\n",
       "      <th>sponsor</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>status</th>\n",
       "      <th>number</th>\n",
       "      <th>subjects_top_term</th>\n",
       "      <th>bill_id</th>\n",
       "      <th>introduced_at</th>\n",
       "      <th>congress</th>\n",
       "      <th>short_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5529</td>\n",
       "      <td>To require the Securities and Exchange Commiss...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.gpo.gov/fdsys/bulkdata/BILLSTATUS/...</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-03-03</td>\n",
       "      <td>False</td>\n",
       "      <td>153</td>\n",
       "      <td>2017-12-11 19:28:52</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1216</td>\n",
       "      <td>Finance and financial sector</td>\n",
       "      <td>hr1216-114</td>\n",
       "      <td>2015-03-03</td>\n",
       "      <td>114</td>\n",
       "      <td>Maker-Taker Conflict of Interest Reform Act of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                     official_title popular_title  \\\n",
       "0  5529  To require the Securities and Exchange Commiss...           NaN   \n",
       "\n",
       "                                                 url  bill_type  status_at  \\\n",
       "0  https://www.gpo.gov/fdsys/bulkdata/BILLSTATUS/...          0 2015-03-03   \n",
       "\n",
       "   by_request  sponsor          updated_at  status  number  \\\n",
       "0       False      153 2017-12-11 19:28:52     4.0    1216   \n",
       "\n",
       "              subjects_top_term     bill_id introduced_at  congress  \\\n",
       "0  Finance and financial sector  hr1216-114    2015-03-03       114   \n",
       "\n",
       "                                         short_title  \n",
       "0  Maker-Taker Conflict of Interest Reform Act of...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bills_info.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "        SELECT \n",
    "        bi.bill_id,\n",
    "        sb.subject,\n",
    "        bi.subjects_top_term,\n",
    "        bi.official_title,\n",
    "        bi.short_title,\n",
    "        sb.bill_ix\n",
    "        FROM subjects sb\n",
    "        INNER JOIN bills bi\n",
    "        ON sb.bill_ix=bi.id\n",
    "        ;\n",
    "        \"\"\"\n",
    "subject_table = pd.read_sql_query(query, engine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bill_id</th>\n",
       "      <th>subject</th>\n",
       "      <th>subjects_top_term</th>\n",
       "      <th>official_title</th>\n",
       "      <th>short_title</th>\n",
       "      <th>bill_ix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hr4764-114</td>\n",
       "      <td>Armed forces and national security</td>\n",
       "      <td>Armed forces and national security</td>\n",
       "      <td>To direct the Secretary of Veterans Affairs to...</td>\n",
       "      <td>Puppies Assisting Wounded Servicemembers (PAWS...</td>\n",
       "      <td>5530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bill_id                             subject  \\\n",
       "0  hr4764-114  Armed forces and national security   \n",
       "\n",
       "                    subjects_top_term  \\\n",
       "0  Armed forces and national security   \n",
       "\n",
       "                                      official_title  \\\n",
       "0  To direct the Secretary of Veterans Affairs to...   \n",
       "\n",
       "                                         short_title  bill_ix  \n",
       "0  Puppies Assisting Wounded Servicemembers (PAWS...     5530  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_table.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "        SELECT \n",
    "        bi.bill_id,\n",
    "        sm.text AS summary_text,\n",
    "        bt.text AS full_text,\n",
    "\n",
    "        bi.subjects_top_term,\n",
    "        bi.official_title,\n",
    "        bi.short_title,\n",
    "\n",
    "        bv.code,\n",
    "        sm.as as summary_as,\n",
    "        sm.date as summary_date,\n",
    "        sm.bill_ix\n",
    "        FROM summaries sm\n",
    "        \n",
    "        INNER JOIN bill_text bt\n",
    "        ON sm.bill_ix=bt.bill_ix\n",
    "        \n",
    "        INNER JOIN bill_versions bv\n",
    "        ON bv.id=bt.bill_version_id\n",
    "        \n",
    "        INNER JOIN bills bi\n",
    "        ON sm.bill_ix=bi.id\n",
    "        ;\n",
    "        \"\"\"\n",
    "bill_join = pd.read_sql_query(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bill_id</th>\n",
       "      <th>summary_text</th>\n",
       "      <th>full_text</th>\n",
       "      <th>subjects_top_term</th>\n",
       "      <th>official_title</th>\n",
       "      <th>short_title</th>\n",
       "      <th>code</th>\n",
       "      <th>summary_as</th>\n",
       "      <th>summary_date</th>\n",
       "      <th>bill_ix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hr1617-113</td>\n",
       "      <td>Emergency Jobs to Restore the American Dream A...</td>\n",
       "      <td>&lt;?xml-stylesheet type=\"text/xsl\" href=\"billres...</td>\n",
       "      <td>Labor and employment</td>\n",
       "      <td>To create an emergency jobs program that will ...</td>\n",
       "      <td>Emergency Jobs to Restore the American Dream Act</td>\n",
       "      <td>IH</td>\n",
       "      <td>Introduced in House</td>\n",
       "      <td>2013-04-18</td>\n",
       "      <td>27079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bill_id                                       summary_text  \\\n",
       "0  hr1617-113  Emergency Jobs to Restore the American Dream A...   \n",
       "\n",
       "                                           full_text     subjects_top_term  \\\n",
       "0  <?xml-stylesheet type=\"text/xsl\" href=\"billres...  Labor and employment   \n",
       "\n",
       "                                      official_title  \\\n",
       "0  To create an emergency jobs program that will ...   \n",
       "\n",
       "                                        short_title code           summary_as  \\\n",
       "0  Emergency Jobs to Restore the American Dream Act   IH  Introduced in House   \n",
       "\n",
       "  summary_date  bill_ix  \n",
       "0   2013-04-18    27079  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bill_join.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30669"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bill_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering correct bill version.\n",
    "For each `bill_id` there are multiple versions of bill `full_text` indicated by the `code`, yet only one `summary_text` for each. <br>\n",
    "For simplicitly, I would only like to consider the most recent bill text because that is likely to correspond to the version of the summary text provided. I have tried to determine the ordering of the codes from **most recent** to **least recent** so that I can select the most recent bill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 23795 unique bills being analyzed and 30669 rows in the table\n"
     ]
    }
   ],
   "source": [
    "print('There are {} unique bills being analyzed and {} rows in the table'.format(bill_join.bill_id.nunique(), len(bill_join)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _return_correct_bill_version(df_bills,\n",
    "                                 as_dict=False,\n",
    "                                 code_order=['ENR', 'EAS', 'EAH', 'RS', 'ES', 'PCS', 'EH', 'RH', 'IS', 'IH']):\n",
    "    # To create a 1-to-1 mapping of bill text and summaries\n",
    "    # by choosing most recent bill text version\n",
    "    \n",
    "    num_rows = len(df_bills)\n",
    "    if num_rows == 0:\n",
    "        raise Exception('Oh no! This bill is not in the database.')\n",
    "    elif num_rows > 1:\n",
    "        code = next(i for i in code_order if i in df_bills['code'].unique())\n",
    "        df_bills = df_bills[df_bills['code'] == code]\n",
    "    if as_dict:\n",
    "        return df_bills.iloc[0].to_dict()\n",
    "    else:\n",
    "        return df_bills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_join = bill_join.groupby('bill_id', group_keys=False).apply(lambda x: _return_correct_bill_version(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 23795 unique bills being analyzed and 23825 rows in the table\n"
     ]
    }
   ],
   "source": [
    "print('There are {} unique bills being analyzed and {} rows in the table'.format(bill_join.bill_id.nunique(), len(bill_join)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ERROR:\n",
    "For some reason some of the bill texts do not match their version number and they are not removed from the filtering. <br>\n",
    "**FIX IN DATABASE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_sizes = bill_join.groupby('bill_id').size()\n",
    "duplicates = bill_join[bill_join.bill_id.isin(group_sizes[group_sizes > 1].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 30 duplicates\n"
     ]
    }
   ],
   "source": [
    "print('There are {} duplicates'.format(int(len(duplicates)/2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error is in the `full_text` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bill_id</th>\n",
       "      <th>summary_text</th>\n",
       "      <th>full_text</th>\n",
       "      <th>subjects_top_term</th>\n",
       "      <th>official_title</th>\n",
       "      <th>short_title</th>\n",
       "      <th>code</th>\n",
       "      <th>summary_as</th>\n",
       "      <th>summary_date</th>\n",
       "      <th>bill_ix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29563</th>\n",
       "      <td>hr1026-115</td>\n",
       "      <td>North Country National Scenic Trail Route Adju...</td>\n",
       "      <td>&lt;?xml-stylesheet type=\"text/xsl\" href=\"billres...</td>\n",
       "      <td>Public lands and natural resources</td>\n",
       "      <td>To revise the authorized route of the North Co...</td>\n",
       "      <td>North Country National Scenic Trail Route Adju...</td>\n",
       "      <td>PCS</td>\n",
       "      <td>Reported to House with amendment(s)</td>\n",
       "      <td>2018-05-10</td>\n",
       "      <td>23945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29564</th>\n",
       "      <td>hr1026-115</td>\n",
       "      <td>North Country National Scenic Trail Route Adju...</td>\n",
       "      <td>&lt;?xml-stylesheet type=\"text/xsl\" href=\"billres...</td>\n",
       "      <td>Public lands and natural resources</td>\n",
       "      <td>To revise the authorized route of the North Co...</td>\n",
       "      <td>North Country National Scenic Trail Route Adju...</td>\n",
       "      <td>PCS</td>\n",
       "      <td>Reported to House with amendment(s)</td>\n",
       "      <td>2018-05-10</td>\n",
       "      <td>23945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14170</th>\n",
       "      <td>hr1117-115</td>\n",
       "      <td>(This measure has not been amended since it wa...</td>\n",
       "      <td>&lt;?xml-stylesheet type=\"text/xsl\" href=\"billres...</td>\n",
       "      <td>Emergency management</td>\n",
       "      <td>To require the Administrator of the Federal Em...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ENR</td>\n",
       "      <td>Public Law</td>\n",
       "      <td>2017-10-19</td>\n",
       "      <td>22163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14171</th>\n",
       "      <td>hr1117-115</td>\n",
       "      <td>(This measure has not been amended since it wa...</td>\n",
       "      <td>&lt;?xml-stylesheet type=\"text/xsl\" href=\"billres...</td>\n",
       "      <td>Emergency management</td>\n",
       "      <td>To require the Administrator of the Federal Em...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ENR</td>\n",
       "      <td>Public Law</td>\n",
       "      <td>2017-10-19</td>\n",
       "      <td>22163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          bill_id                                       summary_text  \\\n",
       "29563  hr1026-115  North Country National Scenic Trail Route Adju...   \n",
       "29564  hr1026-115  North Country National Scenic Trail Route Adju...   \n",
       "14170  hr1117-115  (This measure has not been amended since it wa...   \n",
       "14171  hr1117-115  (This measure has not been amended since it wa...   \n",
       "\n",
       "                                               full_text  \\\n",
       "29563  <?xml-stylesheet type=\"text/xsl\" href=\"billres...   \n",
       "29564  <?xml-stylesheet type=\"text/xsl\" href=\"billres...   \n",
       "14170  <?xml-stylesheet type=\"text/xsl\" href=\"billres...   \n",
       "14171  <?xml-stylesheet type=\"text/xsl\" href=\"billres...   \n",
       "\n",
       "                        subjects_top_term  \\\n",
       "29563  Public lands and natural resources   \n",
       "29564  Public lands and natural resources   \n",
       "14170                Emergency management   \n",
       "14171                Emergency management   \n",
       "\n",
       "                                          official_title  \\\n",
       "29563  To revise the authorized route of the North Co...   \n",
       "29564  To revise the authorized route of the North Co...   \n",
       "14170  To require the Administrator of the Federal Em...   \n",
       "14171  To require the Administrator of the Federal Em...   \n",
       "\n",
       "                                             short_title code  \\\n",
       "29563  North Country National Scenic Trail Route Adju...  PCS   \n",
       "29564  North Country National Scenic Trail Route Adju...  PCS   \n",
       "14170                                                NaN  ENR   \n",
       "14171                                                NaN  ENR   \n",
       "\n",
       "                                summary_as summary_date  bill_ix  \n",
       "29563  Reported to House with amendment(s)   2018-05-10    23945  \n",
       "29564  Reported to House with amendment(s)   2018-05-10    23945  \n",
       "14170                           Public Law   2017-10-19    22163  \n",
       "14171                           Public Law   2017-10-19    22163  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-657-3be6a1f998ce>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-657-3be6a1f998ce>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    if duplicates:\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup = duplicates[['full_text', 'code']].reset_index().values\n",
    "dup[:,1] = list(map(lambda x: x.split('bill-stage=\"')[1].split('\"')[0][0], dup[:,1]))\n",
    "dup[:,2] = list(map(lambda x: x[0], dup[:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_ixs = dup[:,0][dup[:,1] != dup[:,2]]\n",
    "good_ixs = dup[:,0][dup[:,1] == dup[:,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 23795 unique bills being analyzed and 23795 rows in the table\n"
     ]
    }
   ],
   "source": [
    "bad_ixs = dup[:,0][dup[:,1] != dup[:,2]]\n",
    "good_ixs = dup[:,0][dup[:,1] == dup[:,2]]\n",
    "bill_join = bill_join[~bill_join.index.isin(bad_ixs)]\n",
    "print('There are {} unique bills being analyzed and {} rows in the table'.format(bill_join.bill_id.nunique(), len(bill_join)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill = bill_join[bill_join.bill_id == 'hr664-115'].iloc[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_id = bill['bill_id']\n",
    "summary_text = bill['summary_text']\n",
    "full_xml = bill['full_text']\n",
    "subject = bill['subjects_top_term']\n",
    "official_title = bill['official_title']\n",
    "short_title = bill['short_title']\n",
    "code = bill['code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hr664-115'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bill_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1789,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stem the Tide of Overdose Prevalence from Opiate Drugs Act of 2017 or as the STOP OD Act of 2017\n",
      "\n",
      "This bill permits the Centers for Disease Control and Prevention (CDC) to award grants: (1) to expand educational efforts to prevent abuse of opioids, which are drugs with effects similar to opium, such as heroin; (2) to promote treatment of persons who abuse opioids; and (3) to promote understanding of addiction.\n",
      "\n",
      "The Department of Health and Human Services (HHS) may award grants to: (1) support first responders carrying and administering naloxone, which is a prescription drug used to rapidly reverse an opioid overdose; (2) establish processes for referral to treatment for opioid abuse; and (3) reimburse for testing for fentanyl in opioid overdoses and reporting the results to the CDC.\n",
      "\n",
      "This bill amends the Controlled Substances Act to impose a fee on persons convicted of drug offenses. Collected amounts are made available for the HHS grants in this bill.\n",
      "\n",
      "Specified agencies must submit to the Office of E-Government and Information Technology of the Office of Management and Budget an inventory of agency data centers and a strategy to consolidate and optimize the data centers.\n",
      "\n",
      "The bill revises reporting requirements for the Department of Defense (DOD) regarding data centers. DOD and the Director of National Intelligence may waive this bill's data center requirements for any national security system.\n",
      "\n",
      "The bill sets forth requirements for the Office of E-Government and Information Technology, including that the office must publish a goal for cost savings and optimization.\n",
      "\n",
      "The bill's provisions regarding data centers are repealed at the start of FY2021.\n"
     ]
    }
   ],
   "source": [
    "print(summary_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTES:\n",
    "The bill `hr664-115` is a shady bill! It has the words \"and other purposes\" in the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_sub = list(subject_table[subject_table.bill_id == 'hr664-115'].subject.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1790,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_sub = list(subject_table[subject_table.bill_id == bill_id].subject.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1791,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Official title: \n",
      "To prevent the abuse of opiates, to improve response and treatment for the abuse of opiates and related overdoses, and for other purposes.\n",
      "\n",
      "Short title: \n",
      "Stem the Tide of Overdose Prevalence from Opiate Drugs Act of 2017\n",
      "\n",
      "Main subject: \n",
      "Health\n",
      "\n",
      "Sub-subjects:\n",
      "Civil actions and liability\n",
      "Computer security and identity theft\n",
      "Computers and information technology\n",
      "Congressional oversight\n",
      "Criminal procedure and sentencing\n",
      "Drug therapy\n",
      "Drug trafficking and controlled substances\n",
      "Drug, alcohol, tobacco use\n",
      "Emergency medical services and trauma care\n",
      "Executive agency funding and structure\n",
      "First responders and emergency personnel\n",
      "Government information and archives\n",
      "Government studies and investigations\n",
      "Health\n",
      "Health personnel\n",
      "Health programs administration and funding\n",
      "Health promotion and preventive care\n",
      "Performance measurement\n"
     ]
    }
   ],
   "source": [
    "print('Official title: \\n{}'.format(official_title))\n",
    "print('\\nShort title: \\n{}'.format(short_title))\n",
    "print('\\nMain subject: \\n{}'.format(subject))\n",
    "print('\\nSub-subjects:')\n",
    "print(*bill_sub, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze full text structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding xml tags.\n",
    "`section` <br>\n",
    "`subsection == (a), (b), ...`  **OR** `paragraph == (1), (2), ...`  \n",
    "`subparagraph == (A), (B), ...`  <br>\n",
    "`clause == (i), (ii), ...`  <br>\n",
    "`subclause == (I), (II), ...`  <br>\n",
    "`item == (aa), (bb), ...` <br>\n",
    "`subitem == (AA), (BB), ...` <br>\n",
    "`subsubitem == (aaa), (bbb), ...` <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "if not duplicates.empty:\n",
    "    print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_rankings = {'bill':0, 'title': 1, 'section':2, 'subsection':3, 'paragraph':4, \n",
    "                'subparagraph':5, 'clause':6, 'subclause':7, 'item':8, 'subitem':9, 'subsubitem':10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_whitespace(sentence_list):    \n",
    "    white_space = list(string.whitespace)[1:]\n",
    "    for ix in range(len(sentence_list)):\n",
    "        for bad_string in white_space:\n",
    "            if bad_string in sentence_list[ix]:\n",
    "                sentence_list[ix] = sentence_list[ix].replace(bad_string, \"\")\n",
    "    return sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bill_from_xml(xml_string):\n",
    "    \n",
    "    xml_string = _remove_whitespace([xml_string])[0]\n",
    "\n",
    "    match = re.search(r'<external-xref(.*?)>', xml_string)\n",
    "    while match:\n",
    "        start, end = (match.start(), match.end())\n",
    "        xml_string = xml_string.replace(xml_string[start:end], \"\")\n",
    "        match = re.search(r'<external-xref(.*?)>', xml_string)\n",
    "    \n",
    "    xml_string = xml_string.replace(\"</external-xref>\", \"\")\n",
    "    \n",
    "    split_xml = xml_string.split(\"<legis-body\")\n",
    "    xml_root = split_xml[0] + \"<legis-body\"\n",
    "    xml_string = split_xml[-1]\n",
    "    \n",
    "    # Close text tag before external-xref and term to avoid loss of information\n",
    "    #xml_string = xml_string.replace(\"<external-xref\", \"</text><external-xref\")\n",
    "    #xml_string = xml_string.replace(\"</external-xref>\", \"</external-xref><text>\")\n",
    "    #xml_string = xml_string.replace(\"<term>\", \"</text><term>\")\n",
    "    #xml_string = xml_string.replace(\"</term>\", \"</term><text>\")\n",
    "\n",
    "    # Closing text tag didn't work because they weren't always embedded in <text>\n",
    "    # Need to go back to this when I understand XML trees better. \n",
    "    xml_string = xml_string.replace(\"<quote>\", \"\")\n",
    "    xml_string = xml_string.replace(\"</quote>\", \"\")\n",
    "    xml_string = xml_string.replace(\"<term>\", \"\")\n",
    "    xml_string = xml_string.replace(\"</term>\", \"\")\n",
    "    \n",
    "    xml_string = xml_root + xml_string\n",
    "    txt_tree = ET.ElementTree(ET.fromstring(xml_string))\n",
    "    txt_root = txt_tree.getroot()\n",
    "\n",
    "    txt_extract = [[ix, elem.tag, elem.text] for ix, elem\n",
    "                   in enumerate(txt_root.iter())]\n",
    "    \n",
    "    return txt_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_extracted_list(txt_extract,\n",
    "                          tag_rankings=None):\n",
    "    \n",
    "    if not tag_rankings:\n",
    "         tag_rankings = {'bill':0, 'title': 1, 'section':2, 'subsection':3, 'paragraph':4, \n",
    "                'subparagraph':5, 'clause':6, 'subclause':7, 'item':8, 'subitem':9, 'subsubitem':10}\n",
    "\n",
    "    txt_df = pd.DataFrame(txt_extract)\n",
    "    txt_df.columns = ['loc_ix', 'tag', 'text']    \n",
    "    txt_df['tag_rank']  = txt_df['tag'].map(tag_rankings)\n",
    "\n",
    "    # Drop pagebreak tag bc it causes errors\n",
    "    txt_df = txt_df[txt_df.tag != 'pagebreak']\n",
    "\n",
    "    # Drop header section and titles\n",
    "    ix_min = txt_df[txt_df['tag']=='legis-body'].index.values[0]+1\n",
    "    txt_df = txt_df.drop(txt_df.index[np.arange(ix_min)])\n",
    "\n",
    "    # Add enumeration to front of each list item\n",
    "    num_ixs = txt_df[txt_df['tag']=='enum']['loc_ix'].values\n",
    "    for ix in num_ixs:\n",
    "        txt_df.loc[ix+1, 'text'] = txt_df.reindex(range(ix,ix+2))['text'].str.cat(sep=' ')\n",
    "        txt_df = txt_df.drop(ix)\n",
    "\n",
    "    ## Concat the quote-blocks\n",
    "    min_ixs = txt_df[txt_df.tag == 'quoted-block']['loc_ix'].values\n",
    "    max_ixs = txt_df[txt_df.tag == 'after-quoted-block']['loc_ix'].values\n",
    "\n",
    "    # Catch quote blocks in quote blocks\n",
    "    if any(min_ixs[1:] < max_ixs[:-1]):\n",
    "        for ix in range(len(min_ixs)-1):\n",
    "            if min_ixs[ix+1] < max_ixs[ix]:\n",
    "                min_ixs = np.delete(min_ixs, ix+1)\n",
    "                max_ixs = np.delete(max_ixs, ix)\n",
    "\n",
    "    for ix_loc in range(len(min_ixs)):\n",
    "        txt_df.loc[min_ixs[ix_loc], 'text'] = txt_df.reindex(range(min_ixs[ix_loc]+1,max_ixs[ix_loc]+1))['text'].str.cat(sep=' ')\n",
    "        txt_df = txt_df.drop(np.arange(min_ixs[ix_loc]+1,max_ixs[ix_loc]+1), errors='ignore')\n",
    "\n",
    "    section_ix = txt_df[txt_df['tag'] == 'section']['loc_ix'].values\n",
    "    # Collapse section text\n",
    "    try:\n",
    "        assert all(txt_df.reindex(section_ix)['tag'] == 'section')\n",
    "        assert all(txt_df.reindex(section_ix+2)['tag'] == 'header')\n",
    "        txt_df.loc[section_ix, 'text'] = txt_df.reindex(section_ix+2)['text'].values\n",
    "        drop_list = np.append(section_ix + 1, section_ix + 2)\n",
    "        txt_df = txt_df.drop(drop_list, errors='ignore')\n",
    "    except: \n",
    "        diff = 1\n",
    "        while section_ix.size !=0:\n",
    "            inds = txt_df.reindex(section_ix+diff).dropna(subset=['loc_ix']).index.values\n",
    "            if inds.size !=0:\n",
    "                txt_df.loc[inds - diff, 'text'] = txt_df.reindex(inds)['text'].values\n",
    "                txt_df = txt_df.drop(inds)\n",
    "                rm_ix = txt_df.reindex(inds-diff)['loc_ix'].values\n",
    "                section_ix = np.array(list(filter(lambda x: x not in rm_ix, section_ix)))\n",
    "            diff += 1\n",
    "\n",
    "    # Collapse subsection text\n",
    "    subsection_ix = txt_df[txt_df['tag'] == 'subsection']['loc_ix'].values\n",
    "    try:\n",
    "        assert all(txt_df.reindex(subsection_ix)['tag'] == 'subsection')\n",
    "        assert all(txt_df.reindex(subsection_ix+2)['tag'] == 'header')\n",
    "        txt_df.loc[subsection_ix, 'text'] = txt_df.reindex(subsection_ix+2)['text'].values\n",
    "        drop_list = np.append(subsection_ix + 1, subsection_ix + 2)\n",
    "        txt_df = txt_df.drop(drop_list, errors='ignore')\n",
    "    except: \n",
    "        diff = 1\n",
    "        while subsection_ix.size !=0:\n",
    "            inds = txt_df.reindex(subsection_ix+diff).dropna(subset=['loc_ix']).index.values\n",
    "            if inds.size !=0:\n",
    "                txt_df.loc[inds - diff, 'text'] = txt_df.reindex(inds)['text'].values\n",
    "                txt_df = txt_df.drop(inds)\n",
    "                rm_ix = txt_df.reindex(inds-diff)['loc_ix'].values\n",
    "                subsection_ix = np.array(list(filter(lambda x: x not in rm_ix, subsection_ix)))\n",
    "            diff += 1\n",
    "\n",
    "    # Concat text between ranked tags\n",
    "    ranked_tags = txt_df.dropna(subset=['tag_rank']).index.values\n",
    "    ranked_tags = np.append(ranked_tags, max(txt_df.index)+1)\n",
    "    for ix in range(len(ranked_tags)-1):\n",
    "        txt_df.loc[ranked_tags[ix], 'text'] = txt_df.reindex(range(ranked_tags[ix],ranked_tags[ix+1]))['text'].str.cat(sep=' ')\n",
    "        txt_df = txt_df.drop(np.arange(ranked_tags[ix]+1,ranked_tags[ix+1]), errors='ignore')\n",
    "\n",
    "    # Remove short title if first section\n",
    "    if 'short title' in txt_df.iloc[0]['text'].lower():\n",
    "        txt_df = txt_df.drop(txt_df.iloc[0]['loc_ix'])\n",
    "\n",
    "    return txt_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the full text parser.\n",
    "Make better except message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select random rows to analyze\n",
    "df = select_random_rows(bill_join, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hr3939-113\n"
     ]
    }
   ],
   "source": [
    "errors = []\n",
    "for ix, bill in df.iterrows(): \n",
    "    xml_string = bill['full_text']\n",
    "    try:\n",
    "        txt_extract = bill_from_xml(xml_string)\n",
    "        txt_df = _clean_extracted_list(txt_extract, tag_rankings=None)\n",
    "    except:\n",
    "        print(bill['bill_id'])\n",
    "        errors.append(bill['bill_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze summary text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize_sentences(txt_string, nlp):\n",
    "    doc = nlp(txt_string)\n",
    "    txt_sent = [sent.string.strip() for sent in doc.sents]\n",
    "    tokens = [token.text for token in doc if not token.is_stop]\n",
    "    return doc, txt_sent\n",
    "\n",
    "def tokenize_summ(summary_text, nlp, short_title):\n",
    "    _, summ_sent = _tokenize_sentences(summary_text, nlp)\n",
    "    if short_title.lower() in summ_sent[0].lower():\n",
    "        summ_sent = summ_sent[1:]\n",
    "    return summ_sent\n",
    "\n",
    "def _apply_text_cleaning(sent):\n",
    "\n",
    "    sent_clean = [_general_text_cleaning(s) for s in sent]\n",
    "    sent_clean = _remove_punct(sent_clean)\n",
    "    sent_clean = _make_lowercase(sent_clean)\n",
    "\n",
    "    return sent_clean\n",
    "\n",
    "def _remove_punct(sentences):\n",
    "    # remove punctuations and special characters\n",
    "    regex = re.compile(r\"[^a-zA-Z0-9]\")\n",
    "    return [regex.sub(\" \", s) for s in sentences]\n",
    "\n",
    "def _make_lowercase(sentences):\n",
    "    return [s.lower() for s in sentences]\n",
    "\n",
    "def _general_text_cleaning(text):\n",
    "\n",
    "    text = re.sub(\"\\'s\", \"\", text)\n",
    "    text = re.sub(\" whats \", \" what is \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(\"can't\", \"can not\", text)\n",
    "    text = re.sub(\"n't\", \" not \", text)\n",
    "    text = re.sub(\"i'm\", \"i am\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'re\", \" are \", text)\n",
    "    text = re.sub(\"\\'d\", \" would \", text)\n",
    "    text = re.sub(\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(\"e\\.g\\.\", \" eg \", text, flags=re.IGNORECASE)\n",
    "\n",
    "    text = re.sub(\"(the[\\s]+|The[\\s]+)?U\\.S\\.A\\.\", \" America \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"(the[\\s]+|The[\\s]+)?United State(s)?\", \" America \", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # remove comma between numbers    \n",
    "    text = re.sub('(?<=[0-9])\\,(?=[0-9])', \"\", text)\n",
    "\n",
    "    text = re.sub('\\$', \" dollar \", text)\n",
    "    text = re.sub('\\%', \" percent \", text)\n",
    "    text = re.sub('\\&', \" and \", text)\n",
    "    \n",
    "    # for hyphenated\n",
    "    text = re.sub(\"[a-zA-Z0-9\\-]*-[a-zA-Z0-9\\-]*\", \"\".join(text.split(\"-\")) , text)\n",
    "    \n",
    "    # the single 's' in this stage is 99% of not clean text, just kill it\n",
    "    text = re.sub(' s ', \" \", text)\n",
    "    \n",
    "    # reduce extra spaces into single spaces\n",
    "    text = re.sub('[\\s]+', \" \", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_sent = tokenize_summ(summary_text, nlp, short_title)\n",
    "summ_sent_clean = _apply_text_cleaning(summ_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare summary and full text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hr664-115'"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bill_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_string = bill['full_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_extract = bill_from_xml(xml_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_extract = bill_from_xml(xml_string)\n",
    "txt_df = _clean_extracted_list(txt_extract, tag_rankings=None)\n",
    "txt_dflow = pd.DataFrame(txt_df[txt_df['tag_rank']>2]['text'], index=txt_df.index)\n",
    "full_sent = list(txt_dflow.replace(np.nan, '', regex=True)['text'].values)\n",
    "full_sent_clean = _apply_text_cleaning(full_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try ROUGE and Fuzzy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def _create_fuzzy_mat(sent_list_1, sent_list_2):\n",
    "    fuzzy_mat = np.zeros([len(sent_list_1), len(sent_list_2)])\n",
    "    for i in range(len(sent_list_1)):\n",
    "        for j in range(len(sent_list_2)):\n",
    "            fuzzy_mat[i][j] = fuzz.ratio(sent_list_1[i], sent_list_2[j])\n",
    "    return fuzzy_mat*0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_rouge_mat(sent_list_1, sent_list_2):\n",
    "    metrics=['rouge-n']\n",
    "    max_n=1\n",
    "    weight_factor=1.2\n",
    "    stemming=True\n",
    "    evaluator = rouge.Rouge(metrics=metrics,\n",
    "                            max_n=max_n,\n",
    "                            limit_length=False,\n",
    "                            alpha=0.5, # Default F1_score\n",
    "                            weight_factor=weight_factor,\n",
    "                            stemming=stemming)\n",
    "    rouge_mat = np.zeros([len(sent_list_1), len(sent_list_2)])\n",
    "    for i in range(len(sent_list_1)):\n",
    "        for j in range(len(sent_list_2)):\n",
    "            rouge_mat[i][j] = evaluator.get_scores(sent_list_2[j], sent_list_1[i])['rouge-1']['f']\n",
    "    return rouge_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_sim_mat(sent_vecs_1, sent_vecs_2,\n",
    "                    embedding_size):\n",
    "    sim_mat = np.zeros([len(sent_vecs_1), len(sent_vecs_2)])\n",
    "    vlen = embedding_size\n",
    "    for i in range(len(sent_vecs_1)):\n",
    "        for j in range(len(sent_vecs_2)):\n",
    "            sim_mat[i][j] = metrics.pairwise.cosine_similarity(sent_vecs_1[i].\n",
    "                                                               reshape(1,\n",
    "                                                                       vlen),\n",
    "                                                               sent_vecs_2[j].\n",
    "                                                               reshape(1,\n",
    "                                                                       vlen))[0,\n",
    "                                                                              0]\n",
    "    return sim_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_mat = _create_rouge_mat(full_sent_clean, summ_sent_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, 9)"
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 14, 65, 13, 99, 76, 95, 45, 39])"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_match = _get_closest_sent_match_ix(rouge_mat)\n",
    "ix_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 13, 14, 39, 45, 65, 76, 95, 99])"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_match = _get_closest_sent_match_ix(rouge_mat)\n",
    "ix_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This bill amends the Controlled Substances Act to impose a fee on persons convicted of drug offenses.'"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ_sent[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(II) provide updates to the Administrator, on a quarterly basis, ofâ€”'"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[full_sent[i] for i in ix_match][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(2) Promotion of treatment and recovery of persons who abuse such substances.',\n",
       " '(1) A State, with grants first being awarded to States with laws in effect that provide for immunity from civil liability for first responders and health professionals who administer naloxone in the course of their duty to counteract opiate overdoses.',\n",
       " '4. Grants for naloxone, training in the administration of naloxone, and testing for fentanyl',\n",
       " '(ii) a multiyear strategy to achieve the consolidation and optimization of the data centers inventoried under clause (i), that includesâ€”',\n",
       " '(III) year-by-year calculations of investment and cost savings for the period beginning on the date of the enactment of this Act and ending on the date set forth in subsection (e), broken down by each year, including a description of any initial costs for data center consolidation and optimization and life cycle cost savings and other improvements, with an emphasis onâ€”',\n",
       " '(II) provide updates to the Administrator, on a quarterly basis, ofâ€”',\n",
       " '(E) monitor the implementation of the data center strategy of each covered agency that is required under paragraph (1)(A)(ii);',\n",
       " '(b) Ensuring cybersecurity standards for data center consolidation and cloud computing',\n",
       " '(1) Administrator The term Administrator means the Administrator of the Office of Electronic Government established under section 3602 of title 44, United States Code (and also known as the Office of E-Government and Information Technology), within the Office of Management and Budget.']"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[full_sent[i] for i in ix_match]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " ' 1  the increase in  1  the increase in fentanylrelated unintentional overdose fatalities presents another lifethreatening scenario for its victims and threatens firstresponders  unintentional overdose fatalities presents another  1  the increase in fentanylrelated unintentional overdose fatalities presents another lifethreatening scenario for its victims and threatens firstresponders  scenario for its victims and threatens  1  the increase in fentanylrelated unintentional overdose fatalities presents another lifethreatening scenario for its victims and threatens firstresponders  ',\n",
       " ' 2  the u s  sentencing commission ',\n",
       " ' a  ought to consider the presence of fentanyl in connection to the illicit distribution of an illicit substance  as a cutting agent  and']"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_sent_clean[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 300\n",
    "path_to_embedding = '../nlp_models/glove.6B/glove.6B.{}d.txt'.format(embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings, embedding_size = _extract_embeddings(path_to_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Health'"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_embeddings(\n",
    "        path_to_embedding='../nlp_models/glove.6B/glove.6B.300d.txt'):\n",
    "    f = open(path_to_embedding, encoding='utf-8')\n",
    "    word_embeddings = {}\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        word_embeddings[word] = coefs\n",
    "    f.close()\n",
    "    embedding_size = list(coefs.shape)[0]\n",
    "    return word_embeddings, embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_vec = _get_full_text_vectors(full_sent_clean, glove_embeddings, embedding_size, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_vec = _get_summary_text_vectors(summ_sent_clean, glove_embeddings, embedding_size, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_closest_sent_match_ix(sim_mat, num_rows=1):\n",
    "\n",
    "    ix_sort = (-sim_mat).argsort(axis=0)\n",
    "    ix_match = ix_sort[0:num_rows, :]\n",
    "    ix_match = np.sort(ix_match.flatten())\n",
    "    ix_match = np.unique(ix_match)\n",
    "    return ix_match  # closest_match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat = _create_sim_mat(full_vec, summ_vec, embedding_size=embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, 9)"
      ]
     },
     "execution_count": 647,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, 9)"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5,   9,  10,  14,  21,  23,  25,  27,  29,  30,  38,  39,  40,\n",
       "        44,  45,  49,  50,  61,  67,  68,  70,  74,  83,  85,  87,  90,\n",
       "        93,  96,  97,  99, 125])"
      ]
     },
     "execution_count": 653,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 655,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ix_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ix_rouge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{11,\n",
       " 12,\n",
       " 13,\n",
       " 24,\n",
       " 31,\n",
       " 42,\n",
       " 46,\n",
       " 47,\n",
       " 53,\n",
       " 57,\n",
       " 59,\n",
       " 65,\n",
       " 72,\n",
       " 76,\n",
       " 78,\n",
       " 92,\n",
       " 95,\n",
       " 103,\n",
       " 109,\n",
       " 114,\n",
       " 126,\n",
       " 131}"
      ]
     },
     "execution_count": 652,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(ix_rouge) - set(ix_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{29, 30, 44, 49, 61, 68, 93, 97}"
      ]
     },
     "execution_count": 651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(ix_good) - set(ix_rouge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_rouge = set_important_label(rouge_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_important_label(sim_mat, set_percentile=95):\n",
    "    ix_good = []\n",
    "    if len(summ_vec) < len(full_vec):\n",
    "        while len(ix_good) < len(summ_vec):\n",
    "            sim_mat_mask = np.where(sim_mat>=np.percentile(sim_mat, set_percentile), 1, 0)\n",
    "            ix_good = np.unique(np.argwhere(sim_mat_mask!=0)[:,0])\n",
    "            set_percentile -= 5\n",
    "    else:\n",
    "        print('summary length {} > {} bill length}'.format(len(summ_vec), len(full_vec)))    \n",
    "        _, ix_good = _get_closest_sent_match_ix(sim_mat, 1)\n",
    "    return ix_good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_full_text_vectors(full_sent_clean, word_embeddings, embedding_size, not_leglove):\n",
    "    full_vec = [_calc_embedding(s, word_embeddings, embedding_size, not_leglove)\n",
    "                for s in full_sent_clean]\n",
    "    return full_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_summary_text_vectors(summ_sent_clean, word_embeddings,\n",
    "                              embedding_size, not_leglove):\n",
    "    summ_vec = [_calc_embedding(s, word_embeddings, embedding_size, not_leglove)\n",
    "                for s in summ_sent_clean]\n",
    "    return summ_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calc_embedding(sen, word_embeddings, embedding_size, not_leglove=True):\n",
    "    if embedding_size is None:\n",
    "        embedding_size = random.choice(list(word_embeddings.values())).shape\n",
    "    if len(sen) != 0:\n",
    "        if not_leglove:\n",
    "            vector = sum([word_embeddings.get(w, np.zeros(embedding_size))\n",
    "                        for w in sen.split()])/(len(sen.split())+0.001)\n",
    "        else:\n",
    "            sen_emb = []\n",
    "            for w in sen.split():\n",
    "                try: \n",
    "                    e = word_embeddings['word_vectors'][word_embeddings['dictionary'][w]]\n",
    "                except: \n",
    "                    e = np.zeros((100,)).shape\n",
    "                sen_emb.append(e)\n",
    "                vector = sum(sen_emb)/(len(sen.split())+0.001)   \n",
    "\n",
    "    else:\n",
    "        vector = np.zeros(embedding_size)\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    print('loading spacy en_core_web_lg')\n",
    "    start_time = time.time()\n",
    "    nlp = spacy.load('en_core_web_lg')\n",
    "    print(\"--- That took a {} seconds ---\".format(time.time() - start_time))\n",
    "\n",
    "    # filter_bills = filter_bills[:50]\n",
    "    # print(filter_bills.columns)\n",
    "    all_data, all_embed_data = aggregate_training_data(filter_bills,\n",
    "                                                       version=version,\n",
    "                                                       path_to_embedding=path_to_embedding,\n",
    "                                                       nlp=nlp,  not_leglove=False)\n",
    "\n",
    "    file_name = 'trainingdata_v{}_leGLOVEemb{}_{}.csv'.format(version, embedding_size, subject)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### determine subjects distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
