{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/melissaferrari/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sqlalchemy\n",
    "import psycopg2\n",
    "import os\n",
    "import numpy\n",
    "import ast\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from lxml import etree\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from io import StringIO, BytesIO\n",
    "import sumy\n",
    "from sqlalchemy import create_engine\n",
    "import seaborn\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress = '114'\n",
    "bill_type = 'hr'\n",
    "default_bill_number = 4764# 1216 #\n",
    "number = default_bill_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get XML doc from filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_path_root = '/Users/melissaferrari/Projects/repo/congress/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_path = '{}{}/bills/{}/{}{}/text-versions/ih/'.format(bill_path_root, \n",
    "                                                          congress, \n",
    "                                                          bill_type, \n",
    "                                                          bill_type, \n",
    "                                                          number)\n",
    "bill_path += 'BILLS-{}{}{}ih/xml'.format(congress, bill_type, number)\n",
    "\n",
    "xml_file = os.listdir(bill_path)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = etree.parse(os.path.join(bill_path, xml_file))\n",
    "string_tree = etree.tostring(tree).decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Query XML from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bill_id = 'hr1216-114'\n",
    "bill_id = '{}{}-{}'.format(bill_type, number, congress)\n",
    "bill_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'melissaferrari'  # add your Postgres username here\n",
    "host = 'localhost'\n",
    "dbname = 'congressional_bills'\n",
    "db = create_engine('postgres://%s%s/%s' % (user, host, dbname))\n",
    "con = None\n",
    "con = psycopg2.connect(database=dbname, user=user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_id = 'hr2914-113'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"SELECT bill_ix, text FROM bill_text bt INNER JOIN bills b ON bt.bill_ix = b.id WHERE b.bill_id = '%s';\" % bill_id\n",
    "query_text_results = pd.read_sql_query(query_text, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_xml = query_text_results.loc[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_xml = query_text_results.iloc[0]['text']\n",
    "tree = etree.fromstring(string_xml)\n",
    "#summarization_result = models.do_summarization(string_xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = etree.fromstring(string_xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_summary = \"\"\"\n",
    "                SELECT * FROM summaries bt INNER JOIN bills b\n",
    "                ON bt.bill_ix = b.id WHERE b.bill_id = '%s';\n",
    "                \"\"\"\n",
    "data = (bill_id,)\n",
    "query_summary_results = pd.read_sql_query(query_summary % data, con)\n",
    "query_summary_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_summary_results.loc[0, 'url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_summary = \"\"\"\n",
    "                SELECT * FROM summaries bt INNER JOIN bills b\n",
    "                ON bt.bill_ix = b.id WHERE b.bill_id = '%s';\n",
    "                \"\"\"\n",
    "data = (bill_id,)\n",
    "query_summary_results = pd.read_sql_query(query_summary % data, con)\n",
    "query_summary_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(query_summary_results.loc[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_info = \"SELECT * FROM bills WHERE bill_id='%s';\" % bill_id\n",
    "query_info_results = pd.read_sql_query(query_info, con)\n",
    "query_info_results.loc[0]\n",
    "#_, official_title, short_title, subjects_top_term = query_info_results.iloc[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_info = \"SELECT bill_id, official_title, short_title, subjects_top_term FROM bills WHERE bill_id='%s';\" % bill_id\n",
    "query_info_results = pd.read_sql_query(query_info, con)\n",
    "query_info_results\n",
    "_, official_title, short_title, subjects_top_term = query_info_results.iloc[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = []\n",
    "for elt in tree.getiterator():\n",
    "    tag = elt.tag\n",
    "    #if '}' in tag:\n",
    "    #   tag = tag.split('}')[1]\n",
    "    tags.append(tag)\n",
    "#tags = list(np.unique(tags))\n",
    "tags;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elt in tree.getiterator('text'):\n",
    "    if elt.text == None:\n",
    "        pass\n",
    "    if elt.tag == None:\n",
    "        pass\n",
    "    if elt.text == 'None':\n",
    "        pass\n",
    "    else:\n",
    "        print(elt.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_single_text_string(tree, tag='text'):\n",
    "    text = \"\"\n",
    "    for elt in tree.getiterator(tag):\n",
    "        if isinstance(elt.text, str):\n",
    "            text += elt.text  + ' '\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = create_single_text_string(tree)\n",
    "bad_string = \"\\n\\t\\t\\t\"\n",
    "if bad_string in full_text:\n",
    "    full_text =full_text.replace(bad_string, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple Gensim tutorial [[LINK](https://rare-technologies.com/text-summarization-with-gensim/)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim #from gensim import summarization #import summarize, keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized = gensim.summarization.summarize(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(summarized)/len(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_text) - len(summarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw = gensim.summarization.keywords(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing ROUGE\n",
    "[GITHUB REPO](https://github.com/Diego999/py-rouge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_info = \"SELECT bill_id, official_title, short_title, subjects_top_term FROM bills WHERE bill_id='%s';\" % bill_id\n",
    "query_info_results = pd.read_sql_query(query_info, con)\n",
    "query_info_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, official_title, short_title, subjects_top_term = query_info_results.iloc[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_summary = \"\"\"\n",
    "                SELECT bill_ix, text FROM summaries bt INNER JOIN bills b\n",
    "                ON bt.bill_ix = b.id WHERE b.bill_id = '%s';\n",
    "                \"\"\"\n",
    "data = (bill_id,)\n",
    "query_summary_results = pd.read_sql_query(query_summary % data, con)\n",
    "\n",
    "query_summary_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_summary = query_summary_results.iloc[0]['text']\n",
    "# Remove summary title\n",
    "to_remove = actual_summary.split('\\n\\n')[0]\n",
    "if to_remove == short_title:\n",
    "    print('REMOVED THE FOLLOWING TEXT: \\n{}'.format(to_remove))\n",
    "else:\n",
    "    print('Did not remove first sentence: \\n{}'.format(to_remove))\n",
    "actual_summary = actual_summary.split(actual_summary.split('\\n\\n')[0] + '\\n\\n')[1]\n",
    "actual_summary = actual_summary.replace('\\n\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFINE ROUGE EVALUATOR\n",
    "evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                        max_n=4,\n",
    "                        limit_length=True,\n",
    "                        length_limit=100,\n",
    "                        length_limit_type='words',\n",
    "                        alpha=0.5, # Default F1_score\n",
    "                        weight_factor=1.2,\n",
    "                        stemming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = evaluator.get_scores(summarized, actual_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    rouge_df = pd.DataFrame(scores).transpose()\n",
    "except:\n",
    "    for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "        results = {key: value[0] for key, value in results[0].items()}\n",
    "        scores[metric] = results\n",
    "    rouge_df = pd.DataFrame(scores).transpose()\n",
    "rouge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_df = np.multiply(rouge_df, 100.0)\n",
    "rouge_df.columns = ['F1', 'P', 'R']\n",
    "rouge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rouge_naive(auto_summarization, actual_summarization, \n",
    "                      metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                      max_n=4, weight_factor=1.2, stemming=True, \n",
    "                      percent=False):\n",
    "    \n",
    "    ## DEFINE ROUGE EVALUATOR\n",
    "    evaluator = rouge.Rouge(metrics=metrics,\n",
    "                            max_n=max_n,\n",
    "                            limit_length=True,\n",
    "                            length_limit=100,\n",
    "                            length_limit_type='words',\n",
    "                            alpha=0.5, # Default F1_score\n",
    "                            weight_factor=weight_factor,\n",
    "                            stemming=stemming)\n",
    "    \n",
    "    scores = evaluator.get_scores(auto_summarization, actual_summarization)\n",
    "    try:\n",
    "        rouge_df = pd.DataFrame(scores).transpose()\n",
    "    except:\n",
    "        for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "            results = {key: value[0] for key, value in results[0].items()}\n",
    "            scores[metric] = results\n",
    "        rouge_df = pd.DataFrame(scores).transpose()\n",
    "    if percent:\n",
    "        rouge_df = np.multiply(rouge_df, 100.0)\n",
    "        rouge_df.columns = ['F1', 'P', 'R']\n",
    "    return rouge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random Summary Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences_full = len(full_text.split('.'))\n",
    "num_sentences_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_summary_percentage = 0.25\n",
    "num_sentences_sum = int(np.round(num_sentences_full*target_summary_percentage))\n",
    "print('To achieve a summary that is approximately {}% we need provide a summary with {} sentences'.format(int(target_summary_percentage*100), num_sentences_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose random set of sentence locs \n",
    "sentence_locs = list(np.arange(1, num_sentences_full+1))\n",
    "sentence_nums = random.sample(sentence_locs, num_sentences_sum)\n",
    "sentence_nums = sorted(sentence_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose random set of sentence locs \n",
    "sentence_locs = list(np.arange(1, num_sentences_full+1))\n",
    "sentence_nums = random.sample(sentence_locs, num_sentences_sum)\n",
    "sentence_nums = sorted(sentence_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_factorial(n):\n",
    "    total = n\n",
    "    n = n-1 \n",
    "    while n >=1:\n",
    "        total = total*n\n",
    "        n = n -1 \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_combinations(n, k):\n",
    "    return int(np.divide(calculate_factorial(n), calculate_factorial(k)*calculate_factorial(n-k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_permutations(n, k):\n",
    "    return int(np.divide(calculate_factorial(n), calculate_factorial(n-k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_combinations = calculate_combinations(num_sentences_full, num_sentences_sum)\n",
    "print('Number of {} combinations from set of {} elements is: {}'.format(num_sentences_sum, num_sentences_full, num_combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_summaries = list(itertools.combinations(full_text.split('.'), num_sentences_sum))\n",
    "random_summaries = ['. '.join(random_summary) for random_summary in random_summaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(random_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge1_random_sum = pd.DataFrame()\n",
    "for ix, random_summary in enumerate(random_summaries):\n",
    "    rouge1_random_sum = rouge1_random_sum.append(apply_rouge_naive(random_summary, actual_summary, \n",
    "                                                                   metrics=['rouge-n'], max_n=1).set_index(pd.Index([ix])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_summ_sentence_locs = list(itertools.combinations(np.arange(1, 18), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge1_random_stats = rouge1_random_sum[['f']].describe()\n",
    "rouge1_random_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "rouge1_random_sum[['f']].hist(ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rouge = rouge1_random_stats.loc['max', 'f']\n",
    "max_rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_rouge = rouge1_random_stats.loc['min', 'f']\n",
    "min_rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_summary_ix = rouge1_random_sum.query('f == {}'.format(max_rouge)).index[0]\n",
    "worst_summary_ix = rouge1_random_sum.query('f == {}'.format(min_rouge)).index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_summary = random_summaries[best_summary_ix]\n",
    "worst_summary = random_summaries[worst_summary_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_summary_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_summ_sentence_locs[worst_summary_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_rouge_naive(best_summary, actual_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_random_summary = pd.DataFrame()\n",
    "for ix, random_summary in enumerate(random_summaries):\n",
    "    rouge_scores = apply_rouge_naive(random_summary, actual_summary, max_n=9)\n",
    "    rouge_scores['random_sum_ix'] = ix\n",
    "    rouge_random_summary = rouge_random_summary.append(rouge_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_random_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_f_summaries = rouge_random_summary[['random_sum_ix', 'f']]\n",
    "rouge_f_summaries = rouge_f_summaries.reset_index()\n",
    "rouge_f_summaries = rouge_f_summaries.rename(columns={'index':'metric'})\n",
    "rouge_f_summaries = rouge_f_summaries.sort_values(['metric', 'random_sum_ix']).reset_index(drop=True)\n",
    "rouge_f_summaries = rouge_f_summaries.set_index(['metric', 'random_sum_ix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_f_summaries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_f_random_stats = rouge_f_summaries.groupby('metric').describe()\n",
    "rouge_f_random_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rouge_f_random_stats.columns = rouge_f_random_stats.columns.get_level_values(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rouge_f_random_stats.iloc[:, rouge_f_random_stats.columns.get_level_values(1)=='max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = list(rouge_f_summaries.index.get_level_values(0).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = pd.DataFrame()\n",
    "tester = rouge_f_summaries.iloc[rouge_f_summaries.index.get_level_values(0) == 'rouge-1', :].copy()\n",
    "tester = tester.reset_index() \n",
    "tester = tester.rename(columns={'f':tester.metric.unique()[0]})\n",
    "del tester['metric']\n",
    "del tester['random_sum_ix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mindex_val in metrics[1:]:\n",
    "    test = rouge_f_summaries.iloc[rouge_f_summaries.index.get_level_values(0) == mindex_val, :].copy()\n",
    "    test = test.reset_index() \n",
    "    test = test.rename(columns={'f':test.metric.unique()[0]})\n",
    "    del test['metric']\n",
    "    del test['random_sum_ix']\n",
    "    tester = tester.merge(test, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 4))\n",
    "seaborn.violinplot(data=tester[tester.columns], ax=ax);\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = tester.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_max = stats.loc[['max']].transpose()\n",
    "rouge_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_rouge = rouge_f_random_stats.iloc[:, rouge_f_random_stats.columns.get_level_values(1) == 'max'].copy()\n",
    "benchmark_rouge.columns = benchmark_rouge.columns.get_level_values(1)\n",
    "del benchmark_rouge.index.name\n",
    "benchmark_rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix, max_val in rouge_max.iterrows():\n",
    "    print('{} ix={} score={}'.format(ix,\n",
    "                                      rouge_f_summaries.query(\"metric == '{}' & f == {}\".format(ix, max_val[0])).reset_index()['random_sum_ix'][0], \n",
    "                                      np.round(max_val[0], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_sort = np.argsort(tester.sum(axis=1))[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(list(tester.loc[idx_sort].sum(axis=1)));\n",
    "ax.set_ylim(bottom=0); ax.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "tester.loc[idx_sort, metrics].reset_index(drop=True).plot(colormap='viridis', ax=ax);\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5)); ax.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_1_ix_sort = list(np.argsort(tester[['rouge-1']], axis=0)[::-1]['rouge-1'].values)\n",
    "rouge_l_ix_sort = list(np.argsort(tester[['rouge-l']], axis=0)[::-1]['rouge-l'].values)\n",
    "rouge_w_ix_sort = list(np.argsort(tester[['rouge-w']], axis=0)[::-1]['rouge-w'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_1_ix_sort[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_l_ix_sort[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_w_ix_sort[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_summaries[1103]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_summaries[1674]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Complexity\n",
    "Now that I have a very minimal model that runs, I need to add complexity to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences_full = len(full_text.split('.'))\n",
    "print('Full text is {} sentences'.format(num_sentences_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_summary_percentage = 0.25\n",
    "num_sentences_sum = int(np.round(num_sentences_full*target_summary_percentage))\n",
    "print('To achieve a summary that is approximately {}% we need provide a summary with {} sentences'.format(int(target_summary_percentage*100), num_sentences_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Latent Semantic Analysis.\n",
    "Combines term freq with SVD (singular value decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Without stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = LsaSummarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumy_lsa_summarized = \"\"\n",
    "full_text_parsed = PlaintextParser.from_string(full_text, Tokenizer('english'))\n",
    "for sentence in summarizer(full_text_parsed.document, num_sentences_sum):\n",
    "    #print(sentence)\n",
    "    #print(' ')\n",
    "    sumy_lsa_summarized += str(sentence) + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumy_lsa_summarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumy_lsa_summarized_rouge = apply_rouge_naive(sumy_lsa_summarized, actual_summary, \n",
    "                                              max_n=4, weight_factor=1.2, stemming=True)\n",
    "sumy_lsa_summarized_rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### With stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'English'\n",
    "stemmer_eng = Stemmer(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = LsaSummarizer(stemmer_eng)\n",
    "summarizer.stop_words = get_stop_words(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumy_lsa_summarized_stemmed = \"\"\n",
    "full_text_parsed = PlaintextParser.from_string(full_text, Tokenizer('english'))\n",
    "for sentence in summarizer(full_text_parsed.document, num_sentences_sum):\n",
    "    #print(sentence)\n",
    "    #print(' ')\n",
    "    sumy_lsa_summarized_stemmed += str(sentence) + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumy_lsa_summarized_stemmed_rouge = apply_rouge_naive(sumy_lsa_summarized_stemmed, actual_summary, \n",
    "                                              max_n=4, weight_factor=1.2, stemming=True)\n",
    "sumy_lsa_summarized_stemmed_rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n\".join(sumy_lsa_summarized_stemmed.split('. ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lex Rank\n",
    "Graphical based text summarizer\n",
    "[SOURCE](https://medium.com/@ondenyi.eric/extractive-text-summarization-techniques-with-sumy-3d3b127a0a32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.summarizers.lex_rank import LexRankSummarizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = LexRankSummarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumy_summarized_lexrank = \"\"\n",
    "full_text_parsed = PlaintextParser.from_string(full_text, Tokenizer('english'))\n",
    "for sentence in summarizer(full_text_parsed.document, num_sentences_sum):\n",
    "    sumy_summarized_lexrank += str(sentence) + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sumy_summarized_lexrank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_rouge_naive(sumy_summarized_lexrank, actual_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Luhn\n",
    "Scored based on freq of most important words. [SOURCE](https://medium.com/@ondenyi.eric/extractive-text-summarization-techniques-with-sumy-3d3b127a0a32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "summarizer = LuhnSummarizer()\n",
    "sumy_summarized_luhn = \"\"\n",
    "for sentence in summarizer(full_text_parsed.document, num_sentences_sum):\n",
    "    sumy_summarized_luhn += str(sentence) + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n\".join(sumy_summarized_luhn.split('. ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_rouge_naive(sumy_summarized_luhn, actual_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text Rank\n",
    "Graph-based summarization technique with keyword extraction.\n",
    "\n",
    "#### MUST also implement via pytextrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "summarizer = TextRankSummarizer()\n",
    "sumy_summarized_textrank = \"\"\n",
    "for sentence in summarizer(full_text_parsed.document, num_sentences_sum):\n",
    "    sumy_summarized_textrank += str(sentence) + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumy_summarized_textrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_rouge_naive(sumy_summarized_textrank, actual_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TextRank with Pytextrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytextrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for ix, sentence in enumerate(full_text.split('.')):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dict = {\"id\":\"1\", \"text\":full_text}\n",
    "text_json = json.dumps(text_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for graf in pytextrank.parse_doc(text_dict):\n",
    "    print(graf._asdict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TextRank with Summa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import summa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TextRank with NLTK.\n",
    "[Source](https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt') # one time execution\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1. Split into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.tokenize.sent_tokenize(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2. GloVe Word Embeddings (vector representation of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract word vectors\n",
    "f = open('../nlp_models/glove.6B/glove.6B.300d.txt', encoding='utf-8')\n",
    "word_embeddings = {}\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 3. Text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuations, numbers and special characters\n",
    "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "\n",
    "# make alphabets lowercase\n",
    "clean_sentences = [s.lower() for s in clean_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sentences[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove stopwords\n",
    "def remove_stopwords(sen):\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords from the sentences\n",
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 4. Vector representation of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "for i in clean_sentences:\n",
    "    if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "        v = np.zeros((100,))\n",
    "    sentence_vectors.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 5. Similarity Matrix.\n",
    "Use cosine similarity to find similarities between sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity matrix\n",
    "sim_mat = np.zeros([len(sentences), len(sentences)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(sim_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 4\n",
    "j = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sentences[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sentences[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(sentence_vectors[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(sentence_vectors[i].reshape(1,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        if i != j:\n",
    "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), \n",
    "                                              sentence_vectors[j].reshape(1,100))[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 6. Apply pagerank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity matrix to graph\n",
    "nx_graph = nx.from_numpy_array(sim_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = nx.pagerank(nx_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 7. Summary extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_summarized_textrank = \"\"\n",
    "for i in range(num_sentences_sum):\n",
    "    nltk_summarized_textrank += str(ranked_sentences[i][1]) + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_summarized_textrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_nltk_textrank = apply_rouge_naive(nltk_summarized_textrank, actual_summary, max_n=9)[['f']]\n",
    "rouge_nltk_textrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.divide(rouge_nltk_textrank['f'].values, benchmark_rouge['max'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Supervised learning approach\n",
    "[Supervised Machine Learning for Summarizaing Legal Documents](http://atour.iro.umontreal.ca/rali/sites/default/files/publis/SupervisedCanAI2010.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_tags = []\n",
    "for elt in tree.getiterator():\n",
    "    tag = elt.tag\n",
    "    if '}' in tag:\n",
    "        tag = tag.split('}')[1]\n",
    "    ordered_tags.append(tag)\n",
    "unique_tags = list(np.unique(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elt in tree.getiterator():\n",
    "    print('{} --> {}'.format(elt.tag, elt.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_tags[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Unsupervised Text Summarization\n",
    "[SOURCE](https://medium.com/jatana/unsupervised-text-summarization-using-sentence-embeddings-adb15ce83db1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = cluster.KMeans(n_clusters=num_sentences_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_output = kmeans.fit(sentence_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = []\n",
    "for j in range(num_sentences_sum):\n",
    "    idx = np.where(kmeans.labels_ == j)[0]\n",
    "    avg.append(np.mean(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest, distances = metrics.pairwise_distances_argmin_min(cluster_centers, \n",
    "                                                           sentence_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordering = sorted(range(num_sentences_sum), key=lambda k: avg[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = ' '.join([sentences[closest[idx]] for idx in ordering])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_rouge_naive(summary, actual_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"SELECT bill_ix, text FROM bill_text;\"\n",
    "query_text_results = pd.read_sql_query(query_text, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_single_text_string(tree, tag='text'):\n",
    "    text = \"\"\n",
    "    for elt in tree.getiterator(tag):\n",
    "        if isinstance(elt.text, str):\n",
    "            text += elt.text  + ' '\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def do_summarization(string_xml):\n",
    "    string_length = len(string_xml)\n",
    "    print('text length: {}'.format(string_length))\n",
    "    tree = etree.fromstring(string_xml)\n",
    "\n",
    "    text = create_single_text_string(tree, tag='text')\n",
    "\n",
    "    summarized = summarization.summarize(text)\n",
    "    print('summary length: {}'.format(len(summarized)))\n",
    "\n",
    "    return summarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_xml = query_text_results.iloc[0]['text']\n",
    "tree = etree.fromstring(string_tree)\n",
    "#summarization_result = models.do_summarization(string_xml)\n",
    "full_text = create_single_text_string(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = etree.parse(os.path.join(bill_path, xml_file))\n",
    "string_tree = etree.tostring(tree).decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text_results['tree'] = query_text_results['text'].apply(etree.fromstring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text_results['full_text'] = query_text_results['tree'].apply(create_single_text_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text_results['tokenize'] = query_text_results['full_text'].apply(sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text_results['num_sentences'] = query_text_results['tokenize'].apply(len).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text_results['num_sentences'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "query_text_results['num_sentences'].hist(bins=100, ax=ax, alpha=0.7);\n",
    "ax.set_yscale('log'); #ax.set_xlim(xmin=0);\n",
    "ax.set_xlabel('Number of Sentences', size=14);\n",
    "ax.set_title('Log Distribution of Bill Length (N=5550)', size=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text_results[query_text_results['num_sentences'] > query_text_results['num_sentences'].describe()['50%']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_bill = query_text_results.loc[query_text_results['num_sentences'] == \n",
    "                                         query_text_results['num_sentences'].describe()['max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_bill_ix = query_text_results.loc[query_text_results['num_sentences'] == \n",
    "                                         query_text_results['num_sentences'].describe()['max'], 'bill_ix'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_bill = query_text_results.loc[query_text_results['num_sentences'] == \n",
    "                                      query_text_results['num_sentences'].describe()['max'], 'tokenize'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text_results.loc[query_text_results['num_sentences'] == \n",
    "                                      query_text_results['num_sentences'].describe()['max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "official_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_bill_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_info = \"SELECT bill_id, official_title, short_title, subjects_top_term FROM bills WHERE id='%s';\" % longest_bill_ix\n",
    "query_info_results = pd.read_sql_query(query_info, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_id, official_title, short_title, subjects_top_term = query_info_results.loc[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_summary = \"\"\"\n",
    "                SELECT bill_ix, text FROM summaries bt INNER JOIN bills b\n",
    "                ON bt.bill_ix = b.id WHERE b.bill_id = '%s';\n",
    "                \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (bill_id,)\n",
    "query_summary_results = pd.read_sql_query(query_summary % data, con)\n",
    "query_summary_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " query_summary_results.loc[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dict = query_summary_results.loc[0].to_doct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = etree.fromstring(long_bill.loc[25, 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_tags = []\n",
    "for elt in tree.getiterator():\n",
    "    tag = elt.tag\n",
    "    if '}' in tag:\n",
    "        tag = tag.split('}')[1]\n",
    "    ordered_tags.append(tag)\n",
    "unique_tags = list(np.unique(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_tags[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sent = nltk.tokenize.sent_tokenize(full_text)\n",
    "summ_sent = nltk.tokenize.sent_tokenize(actual_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat = np.zeros([len(full_sent), len(summ_sent)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sent_clean = pd.Series(full_sent).str.replace(\"[^a-zA-Z]\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_sent_clean = pd.Series(summ_sent).str.replace(\"[^a-zA-Z]\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make alphabets lowercase\n",
    "summ_sent_clean = [s.lower() for s in summ_sent_clean]\n",
    "full_sent_clean = [s.lower() for s in full_sent_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "# function to remove stopwords\n",
    "def remove_stopwords(sen):\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords from the sentences\n",
    "summ_sent_clean = [remove_stopwords(r.split()) for r in summ_sent_clean]\n",
    "full_sent_clean = [remove_stopwords(r.split()) for r in full_sent_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_sentence_vectors = []\n",
    "for i in summ_sent_clean:\n",
    "    if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "        v = np.zeros((100,))\n",
    "    summ_sentence_vectors.append(v)\n",
    "full_sentence_vectors = []\n",
    "for i in full_sent_clean:\n",
    "    if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "        v = np.zeros((100,))\n",
    "    full_sentence_vectors.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(sim_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat[5][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(full_sentence_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(full_sent)):\n",
    "    for j in range(len(summ_sent)):\n",
    "            sim_mat[i][j] = cosine_similarity(full_sentence_vectors[i].reshape(1,100), \n",
    "                                              summ_sentence_vectors[j].reshape(1,100))[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sim_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_match = np.argmax(sim_mat, axis=0)\n",
    "ix_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat[8, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat[5, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_match = [full_sent[i] for i in ix_match]\n",
    "closest_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clean = [clean(doc).split() for doc in doc_complete]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "get_ipython().config.get('IPKernelApp', {})['parent_appname'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_single_text_string(tree, tag='text'):\n",
    "    text = \"\"\n",
    "    for elt in tree.getiterator(tag):\n",
    "        if isinstance(elt.text, str):\n",
    "            text += elt.text  + ' '\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_text = create_single_text_string(tree)\n",
    "bad_string = \"\\n\\t\\t\\t\"\n",
    "if bad_string in full_text:\n",
    "    full_text =full_text.replace(bad_string, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elt in tree.getiterator('text'):\n",
    "    print(elt.tag, elt.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = full_text.replace(\"\\t\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(full_text)\n",
    "sentences = [sent.string.strip() for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sent = nltk.tokenize.sent_tokenize(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ent = pd.DataFrame()\n",
    "for ix, sentence in enumerate(full_sent):\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        # print(ix, ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "        df_ent = df_ent.append([[ix, ent.text, ent.start_char, ent.end_char, ent.label_]], ignore_index=True)\n",
    "df_ent.columns = ['sentence', 'text', 'start_char', 'end_char', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
